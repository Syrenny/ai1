import torch
import numpy as np
import pandas as pd
import torch.nn as nn
from torch import Tensor
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
from torchvision import datasets, models, transforms
from torch.optim import lr_scheduler
from torchvision.transforms import ToTensor
from torchvision.models import ResNet50_Weights, resnet50
from torch.utils.data import DataLoader, random_split, Dataset
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from torchmetrics import Precision, StatScores
from torchmetrics.classification import MulticlassStatScores


class MyDataset(Dataset):
    def __init__(self, subset, transform=None):
        self.subset = subset
        self.transform = transform
        
    def __getitem__(self, index):
        x, y = self.subset[index]
        if self.transform:
            x = self.transform(x)
        return x, y
        
    def __len__(self):
        return len(self.subset)


# Define the batch size
batch_size = 64

# Define the transformation for the images
transform = {
    "train": transforms.Compose([
        transforms.Resize((224,224)),
        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                             std=[0.229, 0.224, 0.225])]),
    "test": transforms.Compose([
        transforms.Resize((224,224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                             std=[0.229, 0.224, 0.225])])
}

# Load the dataset
dataset = ImageFolder('simpsons_dataset')
dataset_length = len(dataset)
train_set, test_set = random_split(dataset, [int(dataset_length * 0.8), dataset_length - int(dataset_length * 0.8)])
train_set = MyDataset(train_set, transform=transform["train"])
test_set = MyDataset(test_set, transform=transform["test"])
loader = {
    "train": DataLoader(train_set, batch_size=batch_size, shuffle=True),
    "test": DataLoader(test_set, batch_size=batch_size, shuffle=True)
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(device)
for param in resnet.parameters():
    param.requires_grad = False
print(dataset.classes)
num_classes = len(dataset.classes)
resnet.fc = nn.Sequential(
               nn.Linear(2048, 128),
               nn.ReLU(inplace=True),
               nn.Linear(128, num_classes)).to(device)
# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(resnet.fc.parameters(), lr=0.0001, momentum=0.9)

# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

# Train the model
num_epochs = 15
loss_history = []
accuracy_history = []
precision_history = []
recall_history = []
torch.backends.cudnn.benchmark = True
for epoch in range(num_epochs):
  # CM = pd.DataFrame(np.zeros((num_classes, num_classes), dtype=int), 
  #                        index=dataset.classes, columns=dataset.classes)
  # CM = confusion_matrix(np.zeros(num_classes, 0), np.zeros(num_classes, 0))
  stat_scores = StatScores(task="multiclass", num_classes=num_classes)
  running_loss = 0.0
  running_corrects = 0
  total = 0
  for phase in ["train", "test"]:
    print("Running: ", phase)
    for i, (inputs, labels) in enumerate(loader[phase]):
      inputs = inputs.to(device)
      labels = labels.to(device)
      outputs = resnet(inputs)
      if phase == "train":
        optimizer.zero_grad()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        total += labels.size(0)
        running_loss += loss.item() * inputs.size(0)
      if phase == "test":
        stat_scores.update
        print("Testing StatScores...")
        stat_scores(outputs.cpu(), labels.cpu())
      if ((i + 1) % 100) == 0:
        print("Batch number: ", i + 1)
        if phase == "test":
          print("StatScores result: ", stat_scores)
      # _, predicted = torch.max(outputs.data, 1)

      # threshold = 0.9
      '''if phase == "test":
        for i, label in enumerate(labels):
          CM[label] += (outputs >= threshold)'''
      
      '''print(predicted, labels)
      CM+=confusion_matrix(labels.cpu(), predicted.cpu())
      print(CM)'''
      # exp_lr_scheduler.step()
      # metric = MulticlassStatScores(num_classes=num_classes, average='macro')
      # metric(predicted.cpu(), labels.data.cpu())
      # running_corrects += torch.sum(predicted == labels.data)
      # Calculating batch accuracy:
      # correct_batch_counts = predicted.eq(labels.data.view_as(predicted))
      # Convert correct_counts to float and then compute the mean
      # accuracy = torch.mean(correct_batch_counts.type(torch.FloatTensor))

      #  precision = Precision(task="multiclass", average='macro', num_classes=num_classes)
      # precision(predicted.cpu(), labels.data.cpu())
      # calculating recall and precision
      
      # if ((i + 1) % 100) == 0:
      #   print("Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}, Recall: {:.4f}, Precision: {:.4f}".format(i + 1, loss.item(), accuracy, recall, precision))

    epoch_loss = running_loss / len(train_set)
    epoch_accuracy = running_corrects / total
    loss_history.append(epoch_loss)
    accuracy_history.append(epoch_accuracy)
    print(f'Epoch {epoch+1} loss: {epoch_loss:.3f} accuracy: {epoch_accuracy:.3f}')

# Save the trained model
torch.save(resnet.state_dict(), 'model.pth')

# Plot the loss history
plt.plot(loss_history)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

# Plot the accuracy history
plt.plot(accuracy_history)
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()
